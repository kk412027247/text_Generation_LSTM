{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation With LSTM Recurrent Neural Networks in Python with Keras\n",
    "> https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small LSTM Network to Generate Text for Alice in Wonderland\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  148574\n",
      "Total Vocab:  46\n",
      "Total Patterns:  148474\n",
      "Epoch 1/20\n",
      "1157/1160 [============================>.] - ETA: 0s - loss: 2.9336\n",
      "Epoch 1: loss improved from inf to 2.93304, saving model to weights-improvement-01-2.9330.hdf5\n",
      "1160/1160 [==============================] - 17s 11ms/step - loss: 2.9330\n",
      "Epoch 2/20\n",
      "1158/1160 [============================>.] - ETA: 0s - loss: 2.7089\n",
      "Epoch 2: loss improved from 2.93304 to 2.70851, saving model to weights-improvement-02-2.7085.hdf5\n",
      "1160/1160 [==============================] - 13s 11ms/step - loss: 2.7085\n",
      "Epoch 3/20\n",
      "1158/1160 [============================>.] - ETA: 0s - loss: 2.6061\n",
      "Epoch 3: loss improved from 2.70851 to 2.60612, saving model to weights-improvement-03-2.6061.hdf5\n",
      "1160/1160 [==============================] - 12s 11ms/step - loss: 2.6061\n",
      "Epoch 4/20\n",
      "1157/1160 [============================>.] - ETA: 0s - loss: 2.5401\n",
      "Epoch 4: loss improved from 2.60612 to 2.54022, saving model to weights-improvement-04-2.5402.hdf5\n",
      "1160/1160 [==============================] - 12s 10ms/step - loss: 2.5402\n",
      "Epoch 5/20\n",
      "1159/1160 [============================>.] - ETA: 0s - loss: 2.4859\n",
      "Epoch 5: loss improved from 2.54022 to 2.48584, saving model to weights-improvement-05-2.4858.hdf5\n",
      "1160/1160 [==============================] - 12s 11ms/step - loss: 2.4858\n",
      "Epoch 6/20\n",
      "1157/1160 [============================>.] - ETA: 0s - loss: 2.4321\n",
      "Epoch 6: loss improved from 2.48584 to 2.43207, saving model to weights-improvement-06-2.4321.hdf5\n",
      "1160/1160 [==============================] - 12s 11ms/step - loss: 2.4321\n",
      "Epoch 7/20\n",
      "1159/1160 [============================>.] - ETA: 0s - loss: 2.3860\n",
      "Epoch 7: loss improved from 2.43207 to 2.38605, saving model to weights-improvement-07-2.3860.hdf5\n",
      "1160/1160 [==============================] - 12s 10ms/step - loss: 2.3860\n",
      "Epoch 8/20\n",
      "1156/1160 [============================>.] - ETA: 0s - loss: 2.3414\n",
      "Epoch 8: loss improved from 2.38605 to 2.34183, saving model to weights-improvement-08-2.3418.hdf5\n",
      "1160/1160 [==============================] - 12s 11ms/step - loss: 2.3418\n",
      "Epoch 9/20\n",
      "1159/1160 [============================>.] - ETA: 0s - loss: 2.3004\n",
      "Epoch 9: loss improved from 2.34183 to 2.30039, saving model to weights-improvement-09-2.3004.hdf5\n",
      "1160/1160 [==============================] - 12s 10ms/step - loss: 2.3004\n",
      "Epoch 10/20\n",
      "1158/1160 [============================>.] - ETA: 0s - loss: 2.2596\n",
      "Epoch 10: loss improved from 2.30039 to 2.25965, saving model to weights-improvement-10-2.2597.hdf5\n",
      "1160/1160 [==============================] - 12s 11ms/step - loss: 2.2597\n",
      "Epoch 11/20\n",
      "1155/1160 [============================>.] - ETA: 0s - loss: 2.2228\n",
      "Epoch 11: loss improved from 2.25965 to 2.22326, saving model to weights-improvement-11-2.2233.hdf5\n",
      "1160/1160 [==============================] - 12s 11ms/step - loss: 2.2233\n",
      "Epoch 12/20\n",
      "1160/1160 [==============================] - ETA: 0s - loss: 2.1877\n",
      "Epoch 12: loss improved from 2.22326 to 2.18772, saving model to weights-improvement-12-2.1877.hdf5\n",
      "1160/1160 [==============================] - 12s 10ms/step - loss: 2.1877\n",
      "Epoch 13/20\n",
      "1156/1160 [============================>.] - ETA: 0s - loss: 2.1549\n",
      "Epoch 13: loss improved from 2.18772 to 2.15454, saving model to weights-improvement-13-2.1545.hdf5\n",
      "1160/1160 [==============================] - 12s 10ms/step - loss: 2.1545\n",
      "Epoch 14/20\n",
      "1157/1160 [============================>.] - ETA: 0s - loss: 2.1209\n",
      "Epoch 14: loss improved from 2.15454 to 2.12117, saving model to weights-improvement-14-2.1212.hdf5\n",
      "1160/1160 [==============================] - 12s 10ms/step - loss: 2.1212\n",
      "Epoch 15/20\n",
      "1156/1160 [============================>.] - ETA: 0s - loss: 2.0988\n",
      "Epoch 15: loss improved from 2.12117 to 2.09896, saving model to weights-improvement-15-2.0990.hdf5\n",
      "1160/1160 [==============================] - 12s 11ms/step - loss: 2.0990\n",
      "Epoch 16/20\n",
      "1159/1160 [============================>.] - ETA: 0s - loss: 2.0648\n",
      "Epoch 16: loss improved from 2.09896 to 2.06475, saving model to weights-improvement-16-2.0647.hdf5\n",
      "1160/1160 [==============================] - 12s 10ms/step - loss: 2.0647\n",
      "Epoch 17/20\n",
      "1156/1160 [============================>.] - ETA: 0s - loss: 2.0355\n",
      "Epoch 17: loss improved from 2.06475 to 2.03572, saving model to weights-improvement-17-2.0357.hdf5\n",
      "1160/1160 [==============================] - 13s 11ms/step - loss: 2.0357\n",
      "Epoch 18/20\n",
      "1155/1160 [============================>.] - ETA: 0s - loss: 2.0068\n",
      "Epoch 18: loss improved from 2.03572 to 2.00693, saving model to weights-improvement-18-2.0069.hdf5\n",
      "1160/1160 [==============================] - 13s 11ms/step - loss: 2.0069\n",
      "Epoch 19/20\n",
      "1159/1160 [============================>.] - ETA: 0s - loss: 1.9814\n",
      "Epoch 19: loss improved from 2.00693 to 1.98143, saving model to weights-improvement-19-1.9814.hdf5\n",
      "1160/1160 [==============================] - 13s 11ms/step - loss: 1.9814\n",
      "Epoch 20/20\n",
      "1155/1160 [============================>.] - ETA: 0s - loss: 1.9564\n",
      "Epoch 20: loss improved from 1.98143 to 1.95614, saving model to weights-improvement-20-1.9561.hdf5\n",
      "1160/1160 [==============================] - 12s 11ms/step - loss: 1.9561\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f85a95498b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = to_categorical(dataY)\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" ing hard at alice as\n",
      "he said do.\n",
      "\n",
      "  alice looked at the jury-box, and saw that, in her haste, she\n",
      "ha \"\n",
      "d never leke the mirtle goo of the hadd of the carer whuh the daree                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "\n",
    "# load the network weights\n",
    "filename = \"weights-improvement-19-1.9814.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = np.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
