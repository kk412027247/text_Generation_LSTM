{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 23:34:07.097891: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-11 23:34:07.649132: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/tmd/miniconda3/envs/tf/lib/\n",
      "2023-03-11 23:34:07.649219: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/tmd/miniconda3/envs/tf/lib/\n",
      "2023-03-11 23:34:07.649226: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  148574\n",
      "Total Vocab:  46\n",
      "Total Patterns:  148474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 23:34:34.455530: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 59389600 exceeds 10% of free system memory.\n",
      "2023-03-11 23:34:35.287134: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 59389600 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2317/2320 [============================>.] - ETA: 0s - loss: 2.9758\n",
      "Epoch 1: loss improved from inf to 2.97539, saving model to weights-improvement-01-2.9754-bigger.hdf5\n",
      "2320/2320 [==============================] - 43s 17ms/step - loss: 2.9754\n",
      "Epoch 2/50\n",
      "2318/2320 [============================>.] - ETA: 0s - loss: 2.6209\n",
      "Epoch 2: loss improved from 2.97539 to 2.62086, saving model to weights-improvement-02-2.6209-bigger.hdf5\n",
      "2320/2320 [==============================] - 38s 16ms/step - loss: 2.6209\n",
      "Epoch 3/50\n",
      "2319/2320 [============================>.] - ETA: 0s - loss: 2.4694\n",
      "Epoch 3: loss improved from 2.62086 to 2.46947, saving model to weights-improvement-03-2.4695-bigger.hdf5\n",
      "2320/2320 [==============================] - 41s 18ms/step - loss: 2.4695\n",
      "Epoch 4/50\n",
      "2319/2320 [============================>.] - ETA: 0s - loss: 2.3353\n",
      "Epoch 4: loss improved from 2.46947 to 2.33527, saving model to weights-improvement-04-2.3353-bigger.hdf5\n",
      "2320/2320 [==============================] - 40s 17ms/step - loss: 2.3353\n",
      "Epoch 5/50\n",
      "2319/2320 [============================>.] - ETA: 0s - loss: 2.2192\n",
      "Epoch 5: loss improved from 2.33527 to 2.21909, saving model to weights-improvement-05-2.2191-bigger.hdf5\n",
      "2320/2320 [==============================] - 43s 18ms/step - loss: 2.2191\n",
      "Epoch 6/50\n",
      "2320/2320 [==============================] - ETA: 0s - loss: 2.1300\n",
      "Epoch 6: loss improved from 2.21909 to 2.12999, saving model to weights-improvement-06-2.1300-bigger.hdf5\n",
      "2320/2320 [==============================] - 39s 17ms/step - loss: 2.1300\n",
      "Epoch 7/50\n",
      "2318/2320 [============================>.] - ETA: 0s - loss: 2.0523\n",
      "Epoch 7: loss improved from 2.12999 to 2.05248, saving model to weights-improvement-07-2.0525-bigger.hdf5\n",
      "2320/2320 [==============================] - 37s 16ms/step - loss: 2.0525\n",
      "Epoch 8/50\n",
      "2320/2320 [==============================] - ETA: 0s - loss: 1.9926\n",
      "Epoch 8: loss improved from 2.05248 to 1.99264, saving model to weights-improvement-08-1.9926-bigger.hdf5\n",
      "2320/2320 [==============================] - 42s 18ms/step - loss: 1.9926\n",
      "Epoch 9/50\n",
      "2319/2320 [============================>.] - ETA: 0s - loss: 1.9356\n",
      "Epoch 9: loss improved from 1.99264 to 1.93569, saving model to weights-improvement-09-1.9357-bigger.hdf5\n",
      "2320/2320 [==============================] - 43s 19ms/step - loss: 1.9357\n",
      "Epoch 10/50\n",
      "2320/2320 [==============================] - ETA: 0s - loss: 1.8921\n",
      "Epoch 10: loss improved from 1.93569 to 1.89210, saving model to weights-improvement-10-1.8921-bigger.hdf5\n",
      "2320/2320 [==============================] - 42s 18ms/step - loss: 1.8921\n",
      "Epoch 11/50\n",
      "2318/2320 [============================>.] - ETA: 0s - loss: 1.8496\n",
      "Epoch 11: loss improved from 1.89210 to 1.84957, saving model to weights-improvement-11-1.8496-bigger.hdf5\n",
      "2320/2320 [==============================] - 40s 17ms/step - loss: 1.8496\n",
      "Epoch 12/50\n",
      "2317/2320 [============================>.] - ETA: 0s - loss: 1.8141\n",
      "Epoch 12: loss improved from 1.84957 to 1.81429, saving model to weights-improvement-12-1.8143-bigger.hdf5\n",
      "2320/2320 [==============================] - 42s 18ms/step - loss: 1.8143\n",
      "Epoch 13/50\n",
      "2318/2320 [============================>.] - ETA: 0s - loss: 1.7778\n",
      "Epoch 13: loss improved from 1.81429 to 1.77778, saving model to weights-improvement-13-1.7778-bigger.hdf5\n",
      "2320/2320 [==============================] - 39s 17ms/step - loss: 1.7778\n",
      "Epoch 14/50\n",
      "2319/2320 [============================>.] - ETA: 0s - loss: 1.7480\n",
      "Epoch 14: loss improved from 1.77778 to 1.74800, saving model to weights-improvement-14-1.7480-bigger.hdf5\n",
      "2320/2320 [==============================] - 41s 18ms/step - loss: 1.7480\n",
      "Epoch 15/50\n",
      "2320/2320 [==============================] - ETA: 0s - loss: 1.7198\n",
      "Epoch 15: loss improved from 1.74800 to 1.71983, saving model to weights-improvement-15-1.7198-bigger.hdf5\n",
      "2320/2320 [==============================] - 40s 17ms/step - loss: 1.7198\n",
      "Epoch 16/50\n",
      "2319/2320 [============================>.] - ETA: 0s - loss: 1.6945\n",
      "Epoch 16: loss improved from 1.71983 to 1.69458, saving model to weights-improvement-16-1.6946-bigger.hdf5\n",
      "2320/2320 [==============================] - 40s 17ms/step - loss: 1.6946\n",
      "Epoch 17/50\n",
      "2319/2320 [============================>.] - ETA: 0s - loss: 1.6719\n",
      "Epoch 17: loss improved from 1.69458 to 1.67193, saving model to weights-improvement-17-1.6719-bigger.hdf5\n",
      "2320/2320 [==============================] - 39s 17ms/step - loss: 1.6719\n",
      "Epoch 18/50\n",
      "2320/2320 [==============================] - ETA: 0s - loss: 1.6482\n",
      "Epoch 18: loss improved from 1.67193 to 1.64822, saving model to weights-improvement-18-1.6482-bigger.hdf5\n",
      "2320/2320 [==============================] - 41s 18ms/step - loss: 1.6482\n",
      "Epoch 19/50\n",
      "2320/2320 [==============================] - ETA: 0s - loss: 1.6245\n",
      "Epoch 19: loss improved from 1.64822 to 1.62447, saving model to weights-improvement-19-1.6245-bigger.hdf5\n",
      "2320/2320 [==============================] - 41s 18ms/step - loss: 1.6245\n",
      "Epoch 20/50\n",
      "2319/2320 [============================>.] - ETA: 0s - loss: 1.6051\n",
      "Epoch 20: loss improved from 1.62447 to 1.60514, saving model to weights-improvement-20-1.6051-bigger.hdf5\n",
      "2320/2320 [==============================] - 39s 17ms/step - loss: 1.6051\n",
      "Epoch 21/50\n",
      "2319/2320 [============================>.] - ETA: 0s - loss: 1.5865\n",
      "Epoch 21: loss improved from 1.60514 to 1.58652, saving model to weights-improvement-21-1.5865-bigger.hdf5\n",
      "2320/2320 [==============================] - 40s 17ms/step - loss: 1.5865\n",
      "Epoch 22/50\n",
      "2319/2320 [============================>.] - ETA: 0s - loss: 1.5706\n",
      "Epoch 22: loss improved from 1.58652 to 1.57055, saving model to weights-improvement-22-1.5705-bigger.hdf5\n",
      "2320/2320 [==============================] - 42s 18ms/step - loss: 1.5705\n",
      "Epoch 23/50\n",
      "2319/2320 [============================>.] - ETA: 0s - loss: 1.5537\n",
      "Epoch 23: loss improved from 1.57055 to 1.55360, saving model to weights-improvement-23-1.5536-bigger.hdf5\n",
      "2320/2320 [==============================] - 42s 18ms/step - loss: 1.5536\n",
      "Epoch 24/50\n",
      "2319/2320 [============================>.] - ETA: 0s - loss: 1.5394\n",
      "Epoch 24: loss improved from 1.55360 to 1.53944, saving model to weights-improvement-24-1.5394-bigger.hdf5\n",
      "2320/2320 [==============================] - 41s 17ms/step - loss: 1.5394\n",
      "Epoch 25/50\n",
      "2320/2320 [==============================] - ETA: 0s - loss: 1.5207\n",
      "Epoch 25: loss improved from 1.53944 to 1.52075, saving model to weights-improvement-25-1.5207-bigger.hdf5\n",
      "2320/2320 [==============================] - 40s 17ms/step - loss: 1.5207\n",
      "Epoch 26/50\n",
      "2318/2320 [============================>.] - ETA: 0s - loss: 1.5063\n",
      "Epoch 26: loss improved from 1.52075 to 1.50619, saving model to weights-improvement-26-1.5062-bigger.hdf5\n",
      "2320/2320 [==============================] - 42s 18ms/step - loss: 1.5062\n",
      "Epoch 27/50\n",
      "2319/2320 [============================>.] - ETA: 0s - loss: 1.4942\n",
      "Epoch 27: loss improved from 1.50619 to 1.49414, saving model to weights-improvement-27-1.4941-bigger.hdf5\n",
      "2320/2320 [==============================] - 42s 18ms/step - loss: 1.4941\n",
      "Epoch 28/50\n",
      "2317/2320 [============================>.] - ETA: 0s - loss: 1.4832\n",
      "Epoch 28: loss improved from 1.49414 to 1.48296, saving model to weights-improvement-28-1.4830-bigger.hdf5\n",
      "2320/2320 [==============================] - 41s 18ms/step - loss: 1.4830\n",
      "Epoch 29/50\n",
      "2318/2320 [============================>.] - ETA: 0s - loss: 1.4667\n",
      "Epoch 29: loss improved from 1.48296 to 1.46680, saving model to weights-improvement-29-1.4668-bigger.hdf5\n",
      "2320/2320 [==============================] - 40s 17ms/step - loss: 1.4668\n",
      "Epoch 30/50\n",
      "2320/2320 [==============================] - ETA: 0s - loss: 1.4556\n",
      "Epoch 30: loss improved from 1.46680 to 1.45558, saving model to weights-improvement-30-1.4556-bigger.hdf5\n",
      "2320/2320 [==============================] - 39s 17ms/step - loss: 1.4556\n",
      "Epoch 31/50\n",
      "2320/2320 [==============================] - ETA: 0s - loss: 1.4462\n",
      "Epoch 31: loss improved from 1.45558 to 1.44622, saving model to weights-improvement-31-1.4462-bigger.hdf5\n",
      "2320/2320 [==============================] - 41s 18ms/step - loss: 1.4462\n",
      "Epoch 32/50\n",
      "2318/2320 [============================>.] - ETA: 0s - loss: 1.4340\n",
      "Epoch 32: loss improved from 1.44622 to 1.43409, saving model to weights-improvement-32-1.4341-bigger.hdf5\n",
      "2320/2320 [==============================] - 39s 17ms/step - loss: 1.4341\n",
      "Epoch 33/50\n",
      "2319/2320 [============================>.] - ETA: 0s - loss: 1.4217\n",
      "Epoch 33: loss improved from 1.43409 to 1.42152, saving model to weights-improvement-33-1.4215-bigger.hdf5\n",
      "2320/2320 [==============================] - 40s 17ms/step - loss: 1.4215\n",
      "Epoch 34/50\n",
      "2320/2320 [==============================] - ETA: 0s - loss: 1.4152\n",
      "Epoch 34: loss improved from 1.42152 to 1.41524, saving model to weights-improvement-34-1.4152-bigger.hdf5\n",
      "2320/2320 [==============================] - 40s 17ms/step - loss: 1.4152\n",
      "Epoch 35/50\n",
      "2318/2320 [============================>.] - ETA: 0s - loss: 1.4016\n",
      "Epoch 35: loss improved from 1.41524 to 1.40179, saving model to weights-improvement-35-1.4018-bigger.hdf5\n",
      "2320/2320 [==============================] - 40s 17ms/step - loss: 1.4018\n",
      "Epoch 36/50\n",
      "2317/2320 [============================>.] - ETA: 0s - loss: 1.3959\n",
      "Epoch 36: loss improved from 1.40179 to 1.39589, saving model to weights-improvement-36-1.3959-bigger.hdf5\n",
      "2320/2320 [==============================] - 39s 17ms/step - loss: 1.3959\n",
      "Epoch 37/50\n",
      "2320/2320 [==============================] - ETA: 0s - loss: 1.3884\n",
      "Epoch 37: loss improved from 1.39589 to 1.38837, saving model to weights-improvement-37-1.3884-bigger.hdf5\n",
      "2320/2320 [==============================] - 39s 17ms/step - loss: 1.3884\n",
      "Epoch 38/50\n",
      "2318/2320 [============================>.] - ETA: 0s - loss: 1.3798\n",
      "Epoch 38: loss improved from 1.38837 to 1.37985, saving model to weights-improvement-38-1.3798-bigger.hdf5\n",
      "2320/2320 [==============================] - 40s 17ms/step - loss: 1.3798\n",
      "Epoch 39/50\n",
      "2319/2320 [============================>.] - ETA: 0s - loss: 1.3693\n",
      "Epoch 39: loss improved from 1.37985 to 1.36921, saving model to weights-improvement-39-1.3692-bigger.hdf5\n",
      "2320/2320 [==============================] - 40s 17ms/step - loss: 1.3692\n",
      "Epoch 40/50\n",
      "2318/2320 [============================>.] - ETA: 0s - loss: 1.3642\n",
      "Epoch 40: loss improved from 1.36921 to 1.36440, saving model to weights-improvement-40-1.3644-bigger.hdf5\n",
      "2320/2320 [==============================] - 39s 17ms/step - loss: 1.3644\n",
      "Epoch 41/50\n",
      "2320/2320 [==============================] - ETA: 0s - loss: 1.3552\n",
      "Epoch 41: loss improved from 1.36440 to 1.35525, saving model to weights-improvement-41-1.3552-bigger.hdf5\n",
      "2320/2320 [==============================] - 39s 17ms/step - loss: 1.3552\n",
      "Epoch 42/50\n",
      "2318/2320 [============================>.] - ETA: 0s - loss: 1.3475\n",
      "Epoch 42: loss improved from 1.35525 to 1.34758, saving model to weights-improvement-42-1.3476-bigger.hdf5\n",
      "2320/2320 [==============================] - 38s 16ms/step - loss: 1.3476\n",
      "Epoch 43/50\n",
      "2319/2320 [============================>.] - ETA: 0s - loss: 1.3404\n",
      "Epoch 43: loss improved from 1.34758 to 1.34034, saving model to weights-improvement-43-1.3403-bigger.hdf5\n",
      "2320/2320 [==============================] - 39s 17ms/step - loss: 1.3403\n",
      "Epoch 44/50\n",
      "2317/2320 [============================>.] - ETA: 0s - loss: 1.3359\n",
      "Epoch 44: loss improved from 1.34034 to 1.33601, saving model to weights-improvement-44-1.3360-bigger.hdf5\n",
      "2320/2320 [==============================] - 41s 18ms/step - loss: 1.3360\n",
      "Epoch 45/50\n",
      "2318/2320 [============================>.] - ETA: 0s - loss: 1.3304\n",
      "Epoch 45: loss improved from 1.33601 to 1.33042, saving model to weights-improvement-45-1.3304-bigger.hdf5\n",
      "2320/2320 [==============================] - 42s 18ms/step - loss: 1.3304\n",
      "Epoch 46/50\n",
      "2319/2320 [============================>.] - ETA: 0s - loss: 1.3260\n",
      "Epoch 46: loss improved from 1.33042 to 1.32602, saving model to weights-improvement-46-1.3260-bigger.hdf5\n",
      "2320/2320 [==============================] - 39s 17ms/step - loss: 1.3260\n",
      "Epoch 47/50\n",
      "2318/2320 [============================>.] - ETA: 0s - loss: 1.3178\n",
      "Epoch 47: loss improved from 1.32602 to 1.31786, saving model to weights-improvement-47-1.3179-bigger.hdf5\n",
      "2320/2320 [==============================] - 43s 18ms/step - loss: 1.3179\n",
      "Epoch 48/50\n",
      "2318/2320 [============================>.] - ETA: 0s - loss: 1.3091\n",
      "Epoch 48: loss improved from 1.31786 to 1.30926, saving model to weights-improvement-48-1.3093-bigger.hdf5\n",
      "2320/2320 [==============================] - 40s 17ms/step - loss: 1.3093\n",
      "Epoch 49/50\n",
      "2320/2320 [==============================] - ETA: 0s - loss: 1.3059\n",
      "Epoch 49: loss improved from 1.30926 to 1.30594, saving model to weights-improvement-49-1.3059-bigger.hdf5\n",
      "2320/2320 [==============================] - 43s 19ms/step - loss: 1.3059\n",
      "Epoch 50/50\n",
      "2320/2320 [==============================] - ETA: 0s - loss: 1.2965\n",
      "Epoch 50: loss improved from 1.30594 to 1.29655, saving model to weights-improvement-50-1.2965-bigger.hdf5\n",
      "2320/2320 [==============================] - 41s 18ms/step - loss: 1.2965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f884dbd5340>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Larger LSTM Network to Generate Text for Alice in Wonderland\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = to_categorical(dataY)\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# define the checkpoint\n",
    "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" le, being held up by\n",
      "two guinea-pigs, who were giving it something out of a bottle.\n",
      "they all made a  \"\n",
      "little sorroing to the thing was to large as the court, and the sabbit seemed to be tore to the thing was to large as the court, and the sueen said to the mock turtle, and then said `the mice of the table. \n",
      "she was noo fer and the coor and the bat of the court, and the shget said to the coor, `nd the sueen said to the jury, and the sueen said to the jury, and the sueen said to the jury, and the sueen said to the jury, and the sueen said to the jury, and the sueen said to the jury, and the sueen said to the jury, and the sueen said to the jury, and the sueen said to the jury, and the sueen said to the jury, and the sueen said to the jury, and the sueen said to the jury, and the sueen said to the jury, and the sueen said to the jury, and the sueen said to the jury, and the sueen said to the jury, and the sueen said to the jury, and the sueen said to the jury, and the sueen said to the jury, and the sueen said to the jury, and the sueen said to the jury, and the sueen said to the jury, an\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "\n",
    "# load the network weights\n",
    "filename = \"weights-improvement-47-1.3179-bigger.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = np.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
